{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01707bd4-5e62-4e93-a3a2-08cf611f9173",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aed2509-7d11-4ec7-a5dd-e4d4c5babcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f057ed7-9cae-4620-859a-39a2333b831d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Importing Data\n",
    "\n",
    "For this notebook, I am only using the fiction_sample to try models before running them through all of the other datasets. I had hoped that the models would be tranferable with the results, but the models all ended with varying scores. Below, you will find the models that I've tried and an explanation of why I chose them.\n",
    "\n",
    "In our problem statement, we are building a recommender system that will give the user a recommendation on a book that they should read based on words or phrases they about a story they'd like to read. Ideally, it would be similar to saying \"I want a 19th Century novel about love\" and it may give a recommendation of \"Pride and Prejudice\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ef770cb-0e70-4618-ac38-15b80c3385c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_df = pd.read_csv('./data/fiction_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540b107b-b718-4349-a19e-f4a63bb3f2b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aa96a9-2043-4d85-acb8-c09eba5985fd",
   "metadata": {},
   "source": [
    "* **best_params(pipeline, params, X_train, y_train)**: Reads in a pipeline, parameters, X_train, and y_train set that you've created, performs a GridSearchCV to find the best score and parameters through hypertuning. \n",
    "* **return_gs(pipeline, params, X_train, y_train)**: Returns GridSearch of a given pipeline and parameters\n",
    "* **scores(gs, X_train, y_train, X_test, y_test)**: Using the returned gridsearch, the function will fit the model and perform a train-test-split to evaluate the R2 Train and Test scores.\n",
    "* **predictions(pipeline, X_train, X_test, y_train)**: Returns predictions based on a pipeline and its model\n",
    "* **classification_scores(model, y_test, y_pred)**: Using the predictions, it'll return recall, precision, f1, and accuracy scores for you to evaluate.\n",
    "\n",
    "Note: Functions are reused from [my previous Subreddit project](https://git.generalassemb.ly/lisaliang/project-3.git)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e9e072-eee6-4bf7-9762-b19cf847056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_params(pipeline, params, X_train, y_train):\n",
    "    gs = GridSearchCV(pipeline,\n",
    "                      param_grid = params,\n",
    "                      n_jobs=-1)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    return f'Best Score: {gs.best_score_}, Params: {gs.best_params_}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5dc235e-5b3f-4597-98f8-a797ce233b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_gs(pipeline, params, X_train, y_train):\n",
    "    gs = GridSearchCV(pipeline,\n",
    "                      param_grid = params,\n",
    "                      n_jobs=-1)\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "634f2337-f941-4017-8afa-c7375ed3c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(gs, X_train, y_train, X_test, y_test):\n",
    "    gs.fit(X_train, y_train)\n",
    "    return f'Train Score: {gs.score(X_train, y_train)}, Test Score: {gs.score(X_test, y_test)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0723855c-319b-4f3c-a004-97c8b3432add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(pipeline, X_train, X_test, y_train):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    prediction = pipeline.predict(X_test)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1ddf7ec-a9af-4f2e-bd38-ba094d8f8c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_scores(model, y_test, y_pred):\n",
    "    dataframe = pd.DataFrame(columns = ['Recall', 'Precision', 'F1', 'Accuracy'])\n",
    "    \n",
    "    recall = recall_score(y_test, y_pred, average = 'weighted')\n",
    "    precision = precision_score(y_test, y_pred, average = 'weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average = 'weighted')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    dataframe.loc[model] = [recall, precision, f1, accuracy]\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304144c4-36c6-436f-9794-a56664a963d6",
   "metadata": {},
   "source": [
    "* **my_lemmatizer(text)**: This function lemmatizes inputted text to their dictionary forms. It adds conditions to filter out words with apostrophes or digits so they are done as accurately as possible.\n",
    "\n",
    "Additional: We created a list of English stopwords, contractions, and numbers for the model to remove while it's iterating through the text. These attributes were seen as not adding significance in helping the model distinguish book titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "435649a2-cf14-4f4f-9ac5-a01e0e50106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_lemmatizer(text):\n",
    "    wnet = WordNetLemmatizer()\n",
    "    # exclude words with apostrophes and numbers\n",
    "    return [wnet.lemmatize(w) for w in text.split() if \"'\" not in w and not w.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "366be187-e46e-4126-8608-3dd0da9cc982",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnet = WordNetLemmatizer()\n",
    "lem_stopwords = [wnet.lemmatize(w) for w in stopwords.words('english')]\n",
    "\n",
    "contractions = ['ve', 't', \"'s'\", 'd', 'll', 'm', 're']\n",
    "lem_contractions = [wnet.lemmatize(contraction) for contraction in contractions]\n",
    "\n",
    "numbers = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "lem_numbers = [wnet.lemmatize(num) for num in numbers]\n",
    "\n",
    "lem_stopwords = lem_stopwords + lem_contractions + lem_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6418b6-9b13-4395-b85e-cc2f87e800d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Baseline Accuracy \n",
    "\n",
    "For the baseline accuracy, we see that there are 6722 unique values where Pride and Prejudice has a slight bias towards it. The dataset (along with all the others) are incredibly imbalanced with the number of classes present and the data within those classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76e8afb5-7a7d-4484-9d24-49d91ed02106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pride and Prejudice                                         0.03694\n",
       "Brave New World                                             0.01184\n",
       "Great Expectations                                          0.01102\n",
       "To kill a mockingbird                                       0.00634\n",
       "Alice's Adventures in Wonderland                            0.00580\n",
       "                                                             ...   \n",
       "Chocolate Dipped Death (A Candy Shop Mystery)               0.00002\n",
       "Predator: Concrete Jungle                                   0.00002\n",
       "The Gates of Damascus                                       0.00002\n",
       "His Love Saved Her                                          0.00002\n",
       "Miss Billings Treads the Boards (Signet Regency Romance)    0.00002\n",
       "Name: Title, Length: 6722, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiction_df['Title'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f68ff75-541d-4913-82e6-d682e4ec5e96",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Instantiating X and y\n",
    "X will be the description that we perform the model on and y would be the titles that they could be classified as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18ce2711-2cda-4429-9ddf-c1b7bafd5c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fiction_df['description']\n",
    "y = fiction_df['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f32134b-fbaa-49d5-9e43-b10e999d83e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aef9f2-6363-49c8-8e9a-990d549833aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Multinomial Naive Bayes (and Hypertuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27349af6-102f-445c-9dc1-f49b2b47c547",
   "metadata": {},
   "source": [
    "With Multinomial Naive Bayes, I chose this model for a few reasons:\n",
    "* Efficiency with large datasets\n",
    "* Ability to use text for probability and predictions\n",
    "\n",
    "This model works by assigning documents to classes based an analysis of the content [source](https://towardsdatascience.com/multinomial-na%C3%AFve-bayes-for-documents-classification-and-natural-language-processing-nlp-e08cc848ce6). In doing so, it could take a fragment of text and determine the likelihood that it'll belong to specific class. It was ideal for my intentions since I was parsing through the book descriptions to make these classifications and predictions for my system.\n",
    "\n",
    "For the pipeline, I paired the MNB with a TfidfVectorizer as the latter considers how many times the word appears and its impact in the text. In our EDA, we saw the words, bigrams, and trigrams that were in our datasets. Most were generic enough that they didn't add meaning to our exploration, but not simple enough to be taken out my our lemmatization efforts. I set the stop_words and tokenizer to the special functions that I had created. The max_features was set to 5_000 to allow the model to run.\n",
    "\n",
    "As for the parameters, I used the min_df and max_df to set thresholds to note the required times the word appears to be considered in the tf-idf process. I included an ngram_range to add dimensionality to the GridSearch.\n",
    "\n",
    "Finally, for the MNB, we evaluated the alpha to consider zero probabilities and fit prior that looks at prior knowledge of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1913c227-b2de-4c6b-9acf-aa398ff9e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_pipe = Pipeline([\n",
    "    ('tf', TfidfVectorizer(stop_words = lem_stopwords, \n",
    "                           tokenizer = my_lemmatizer,\n",
    "                           token_pattern = None,\n",
    "                           max_features = 5_000)),\n",
    "    ('mnb', MultinomialNB(alpha = 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1256cbbf-6de1-4651-a259-c315bc39b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_params = {\n",
    "    'tf__min_df': [0.1, 0.25, 0.5, 1.0],\n",
    "    'tf__max_df': [0.25, 0.5, 0.8, 1.0],\n",
    "    'tf__ngram_range': [(1,1), (2,2), (3,3)],\n",
    "    'mnb__alpha': [0.1, 0.25, 0.5, 1],\n",
    "    'mnb__fit_prior': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0392fbc4-f0ec-48d1-bfb7-e4557fe2b2ef",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params(mnb_pipe, mnb_params, X_train, y_train)\n",
    "\n",
    "# this cell runs a long warning, so the output was cleared\n",
    "# Output: \"Best Score: 0.7210666666666666, Params: {'mnb__alpha': 0.1, 'mnb__fit_prior': False, 'tf__max_df': 0.5, 'tf__min_df': 0.1, 'tf__ngram_range': (1, 1)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b03bfd4-ca55-4f06-84c5-99f2db948f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_gs = return_gs(mnb_pipe, mnb_params, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17020be4-e9c4-4073-b5ab-3f4adcc81ec1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores(mnb_gs, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# this cell runs a long warning, so the output was cleared\n",
    "# Output: 'Train Score: 0.7585866666666666, Test Score: 0.70728'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0d58538-943b-4a9c-96ea-96d172c5a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_pred = predictions(mnb_pipe, X_train, X_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8794fa8f-1ac4-4b17-abc1-2a6823c7e404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisaliang/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/lisaliang/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Multinomial Naive Bayes</th>\n",
       "      <td>0.71232</td>\n",
       "      <td>0.580423</td>\n",
       "      <td>0.625477</td>\n",
       "      <td>0.71232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Recall  Precision        F1  Accuracy\n",
       "Multinomial Naive Bayes  0.71232   0.580423  0.625477   0.71232"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_scores('Multinomial Naive Bayes', y_test, mnb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8c2863-bb03-4706-893d-48215672335b",
   "metadata": {},
   "source": [
    "**Evaluation**: The model had an accuracy of 71.23% which meant that it was classifying models correctly 71.2%. However, the model had a low precision score at 58.04% which was what I had hoped to reduce since having false negatives was worst. If users enjoyed one book, they may enjoy a similar book. A false negative would be classifying as one that a user would enjoy, especially since the model is based on the description of a book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc1d2f6-ab9e-4e49-8069-1a27cbb8a0c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Additional Recommendations\n",
    "\n",
    "The only successful model I ran was the Multinomial Naive Bayes. I attempted to run the Random Forest Classification, but only worked for some of my datasets. The Support Vector Classification ran for 8 hours, and ultimately crashed. For anyone who wants to explore similar methods as mine, I highly suggest being put on a cloud service so that you are able to run your models appropriate without the memory affecting your local computer. Below, you can still read about why I wanted to test certain models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32edc60-8446-42f7-8fdd-8c9e9bf4ed13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Random Forest Classification (and Hypertuning)\n",
    "\n",
    "I wanted to explore this model because:\n",
    "* Ability to pinpoint strongest predictors in a model\n",
    "* Training on imbalanced data\n",
    "\n",
    "With the Random Forest Classifier, the ensemble model will take multiple Decision Trees to make the predictions. In doing so, it would alleviate the impact that the imbalanced dataset has as it'll randomly select the subset of variables the Decision Tree uses. The parameters I chose to tune were the n_estimators and max_depth. N_esimators would've evaluated the number of Decision Trees used while max_depth controls how complex the trees are by manipulating the length between the root and leaf nodes.\n",
    "\n",
    "However, one disadvantage is that the model takes a large amount of time to run (similar to Logistic Regression), especially in dealing with multiple decision trees and depth. The memory requirements had caused my computer to crash oftentimes, but occassionally a model successfully ran (go to part 5). For future projects, a cloud service or computer with more available memory capacity should be used to run this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71a1bd39-f29a-4854-9c73-d1d7c40fc77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pipe = Pipeline([\n",
    "    ('tf', TfidfVectorizer(stop_words = lem_stopwords, \n",
    "                           tokenizer = my_lemmatizer,\n",
    "                           token_pattern = None,\n",
    "                           max_features = 1_000)),\n",
    "    ('rfc', RandomForestClassifier(max_features = 1_000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8c223dc-c80b-43e3-a8ab-e03048b0e87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_params = {\n",
    "    'tf__min_df': [0.05, 0.1],\n",
    "    'tf__max_df': [0.5],\n",
    "    'tf__ngram_range': [(1,1)],\n",
    "    'rfc__n_estimators': [100, 200, 300],\n",
    "    'rfc__max_depth': [None, 5, 10, 20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96e8795-5649-4715-93cf-ea2aa6be2539",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params(rfc_pipe, rfc_params, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc86b2-04ca-474e-9b6d-8b5310ad608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs = return_gs(rfc_pipe, rfc_params, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd00ec3b-b444-433f-b87e-96304d8e1771",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores(rfc_gs, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa33ce63-b422-4b16-8832-7cc56744f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pred = predictions(rfc_pipe, X_train, X_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec744f4-965d-4064-aa89-30e376eedc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_scores('Random Forest Classifier', y_test, rfc_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21644bb-3641-4abd-8669-288af4ae446f",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Reasons why I wanted to explore Logistic Regression:\n",
    "* Interpretibility\n",
    "\n",
    "With Logistic Regression, we find the relationship between the features and the target variable by minimizing the loss function. It works well with overfit models as we've seen in the MNB model that we had. This model, I did not actually run once because of the time constraints the regularization would cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01e68ca8-1391-4233-bbdc-073f45bb3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipe = Pipeline([\n",
    "    ('tf', TfidfVectorizer(stop_words = lem_stopwords, \n",
    "                           tokenizer = my_lemmatizer,\n",
    "                           token_pattern = None,\n",
    "                           max_features = 1_000)),\n",
    "    ('lr', LogisticRegression(solver = 'saga'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc66f019-4402-43a1-836b-bb5fdad717b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "    'tf__min_df': [0.05, 0.1, 0.25, 0.5],\n",
    "    'tf__max_df': [0.25, 0.5, 0.8],\n",
    "    'tf__ngram_range': [(1,1), (2,2), (3,3)],\n",
    "    'lr__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "    'lr__C': [0.05, 1.0, 10],\n",
    "    'lr__class_weight': [None, 'balanced']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d4ab3f-825a-4c2a-afa1-949245a01179",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params(lr_pipe, lr_params, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6cafb2-3008-451d-b60c-433229bff4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_gs = return_gs(lr_pipe, lr_params, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6992d34-db6f-431b-b05e-bde84b14347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores(lr_gs, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e575c413-a711-4eff-9d48-6d633e44e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pred = predictions(lr_pipe, X_train, X_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a8ce93-f91d-423d-83b0-fa86acc41cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_scores('Logistic Regression', y_test, lr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64892872-f9a2-432e-98e2-6111a99130d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Support Vector Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c4ee46-57eb-4616-b85d-46cdc5a3fca3",
   "metadata": {},
   "source": [
    "Originally, I wanted to evaluate the Support Vector Classification for its ability on dataset dimensionality. It finds the closest match of a data point to the curve it creates, so it works well with non-linear relationships.\n",
    "\n",
    "However, as I read more and ran some models, I would not recommend using this further as its features doesn't align with what I want the model to do and the dataset we are given. The Support Vector is not suitable for large datasets and noise which all my datasets have since they are classifying thousands of classes that overlap. Typically, the Support Vector will underperform too by favoring the majority class in imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00ab7aa5-3881-41bc-bfbd-2eabd773566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_pipe = Pipeline([\n",
    "    ('tf', TfidfVectorizer(stop_words = lem_stopwords, \n",
    "                           tokenizer = my_lemmatizer,\n",
    "                           token_pattern = None,\n",
    "                           max_features = 1_000)),\n",
    "    ('sv', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a60c2f8-00ae-49cb-95f0-d181ba9c565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_params = {\n",
    "    'tf__min_df': [0.05, 0.1, 0.25, 0.5],\n",
    "    'tf__max_df': [0.25, 0.5, 0.8],\n",
    "    'tf__ngram_range': [(1,1), (2,2), (3,3)],\n",
    "    'sv__C': [0.5, 1, 10],\n",
    "    'sv__kernel': ['linear', 'poly', 'rbf'],\n",
    "    'sv__class_weight': [None, 'balanced']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74810f46-2d14-4791-a0db-4da49e4587b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params(sv_pipe, sv_params, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da9f8b-0d5e-4b3b-a43d-21b475250bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_gs = return_gs(sv_pipe, sv_params, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153c6cf0-7028-4f26-8cbb-7e374c7fe59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores(sv_gs, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d18e50a-c9b8-4f28-9be2-0a41c7fbd1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_pred = predictions(sv_pipe, X_train, X_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c20a3-37e3-44bc-bbc6-b6496f596973",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_scores('Support Vector', y_test, sv_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
